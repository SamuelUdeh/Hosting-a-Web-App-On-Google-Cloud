gcloud auth list
gcloud config list project

# To set the region and zone for hosting the web app

gcloud config set compute/zone "us-west1-c"
export ZONE=$(gcloud config get compute/zone)

gcloud config set compute/region "us-west1"
export REGION=$(gcloud config get compute/region)

# To Enable Compute Engine API

gcloud services enable compute.googleapis.com

# To create a cloud storage bucket for housing the built code as well as the start up scripts

gsutil mb gs://fancy-store-$DEVSHELL_PROJECT_ID

# To clone the source repository

git clone https://github.com/googlecodelabs/monolith-to-microservices.git

# To change the directory

cd ~/monolith-to-microservices

# To run the initial buid of the project
./setup.sh

# T o ensure cloud shell is running a Nodejs application
nvm install --lts

# To run the application by switching to the microservices directory
cd microservices
npm start
# Preview your application by clicking the web preview icon then selecting Preview on port 8080,close the window by clicking CTRL +C

# Now to create the compute engine instance, we would first create a startup scripts to configure instances
# The startup script will be used to instruct the instance what to do each time it is started, hence making it automatic
touch ~/monolith-to-microservices/startup-script.sh

# Click Open Editor in the Cloud Shell ribbon to open the Code Editor
# Navigate to the monolith-to-microservices folder
# Add the following code to the startup-script.sh file

#!/bin/bash

# Install logging monitor. The monitor will automatically pick up logs sent to
# syslog.
curl -s "https://storage.googleapis.com/signals-agents/logging/google-fluentd-install.sh" | bash
service google-fluentd restart &

# Install dependencies from apt
apt-get update
apt-get install -yq ca-certificates git build-essential supervisor psmisc

# Install nodejs
mkdir /opt/nodejs
curl https://nodejs.org/dist/v16.14.0/node-v16.14.0-linux-x64.tar.gz | tar xvzf - -C /opt/nodejs --strip-components=1
ln -s /opt/nodejs/bin/node /usr/bin/node
ln -s /opt/nodejs/bin/npm /usr/bin/npm

# Get the application source code from the Google Cloud Storage bucket.
mkdir /fancy-store
gsutil -m cp -r gs://fancy-store-hosting-a-web-app-416702/monolith-to-microservices/microservices/* /fancy-store/

# Install app dependencies.
cd /fancy-store/
npm install

# Create a nodeapp user. The application will run as this user.
useradd -m -d /home/nodeapp nodeapp
chown -R nodeapp:nodeapp /opt/app

# Configure supervisor to run the node app.
cat >/etc/supervisor/conf.d/node-app.conf << EOF
[program:nodeapp]
directory=/fancy-store
command=npm start
autostart=true
autorestart=true
user=nodeapp
environment=HOME="/home/nodeapp",USER="nodeapp",NODE_ENV="production"
stdout_logfile=syslog
stderr_logfile=syslog
EOF

supervisorctl reread
supervisorctl update

# Save the file and return to the Cloud Shell Terminal run the following to copy the startup-script.sh file into your bucket:
gsutil cp ~/monolith-to-microservices/startup-script.sh gs://fancy-store-$DEVSHELL_PROJECT_ID

# To copy the cloned code into the bucket
cd ~
rm -rf monolith-to-microservices/*/node_modules
gsutil -m cp -r monolith-to-microservices gs://fancy-store-$DEVSHELL_PROJECT_ID/

# To deploy the backend instance
gcloud compute instances create backend \
    --zone=$ZONE \
    --machine-type=e2-standard-2 \
    --tags=backend \
   --metadata=startup-script-url=https://storage.googleapis.com/fancy-store-$DEVSHELL_PROJECT_ID/startup-script.sh

   # To Configure a connection to the backend
   gcloud compute instances list
   # To Copy the External IP for the backend
   # In the Cloud Shell Explorer, navigate to monolith-to-microservices > react-app
   # In the Code Editor, select View > Toggle Hidden Files in order to see the .env file
   # In the .env file, replace localhost with your [BACKEND_ADDRESS]
   # In Cloud Shell, run the following to rebuild react-app, which will update the frontend code:

   cd ~/monolith-to-microservices/react-app
npm install && npm run-script build

# Then copy the application code into the Cloud Storage bucket:
cd ~
rm -rf monolith-to-microservices/*/node_modules
gsutil -m cp -r monolith-to-microservices gs://fancy-store-$DEVSHELL_PROJECT_ID/

# To deploy the frontend instance
gcloud compute instances create frontend \
    --zone=$ZONE \
    --machine-type=e2-standard-2 \
    --tags=frontend \
    --metadata=startup-script-url=https://storage.googleapis.com/fancy-store-$DEVSHELL_PROJECT_ID/startup-script.sh

    # Configure the network
    gcloud compute firewall-rules create fw-fe \
    --allow tcp:8080 \
    --target-tags=frontend

    gcloud compute firewall-rules create fw-be \
    --allow tcp:8081-8082 \
    --target-tags=backend

    # The website should now be fully functional

    # In order to navigate to the external IP of the frontend, you need to know the address. Run the following and look for the EXTERNAL_IP of the frontend instance:
    gcloud compute instances list

    # Wait 3 minutes and then open a new browser tab and browse to http://[FRONTEND_ADDRESS]:8080 to access the website, where [FRONTEND_ADDRESS] is the frontend EXTERNAL_IP determined above
    # Create managed instance groups to allow the application to scale
    # First, stop both instances:
    gcloud compute instances stop frontend --zone=$ZONE
    gcloud compute instances stop backend --zone=$ZONE

    # Then, create the instance template from each of the source instances:
    gcloud compute instance-templates create fancy-fe \
    --source-instance-zone=$ZONE \
    --source-instance=frontend

    gcloud compute instance-templates create fancy-be \
    --source-instance-zone=$ZONE \
    --source-instance=backend

    # To confirm the instances were created
    gcloud compute instance-templates list

    # With the instance templates created, delete the backend vm to save resource space:
    gcloud compute instances delete backend --zone=$ZONE

    # Create two managed instance group one for the frontend and one for the backend
    gcloud compute instance-groups managed create fancy-fe-mig \
    --zone=$ZONE \
    --base-instance-name fancy-fe \
    --size 2 \
    --template fancy-fe

    gcloud compute instance-groups managed create fancy-be-mig \
    --zone=$ZONE \
    --base-instance-name fancy-be \
    --size 2 \
    --template fancy-be

    # For your application, the frontend microservice runs on port 8080, and the backend microservice runs on port 8081 for orders and port 8082 for products:
    gcloud compute instance-groups set-named-ports fancy-fe-mig \
    --zone=$ZONE \
    --named-ports frontend:8080

    gcloud compute instance-groups set-named-ports fancy-be-mig \
    --zone=$ZONE \
    --named-ports orders:8081,products:8082

    # To improve the availability of the application itself and to verify it is responding, configure an autohealing policy for the managed instance groups.

    # Create a health check that repairs the instance if it returns "unhealthy" 3 consecutive times for the frontend and backend:
    gcloud compute health-checks create http fancy-fe-hc \
    --port 8080 \
    --check-interval 30s \
    --healthy-threshold 1 \
    --timeout 10s \
    --unhealthy-threshold 3

    gcloud compute health-checks create http fancy-be-hc \
    --port 8081 \
    --request-path=/api/orders \
    --check-interval 30s \
    --healthy-threshold 1 \
    --timeout 10s \
    --unhealthy-threshold 3

    # Create a firewall rule to allow the health check probes to connect to the microservices on ports 8080-8081:
    gcloud compute firewall-rules create allow-health-check \
    --allow tcp:8080-8081 \
    --source-ranges 130.211.0.0/22,35.191.0.0/16 \
    --network default

    # Apply the health checks to their respective services:
    gcloud compute instance-groups managed update fancy-fe-mig \
    --zone=$ZONE \
    --health-check fancy-fe-hc \
    --initial-delay 300

    gcloud compute instance-groups managed update fancy-be-mig \
    --zone=$ZONE \
    --health-check fancy-be-hc \
    --initial-delay 300

    # Create HTTP(S) load balancer
    # Create health checks that will be used to determine which instances are capable of serving traffic for each service:

    gcloud compute http-health-checks create fancy-fe-frontend-hc \
  --request-path / \
  --port 8080

  gcloud compute http-health-checks create fancy-be-orders-hc \
  --request-path /api/orders \
  --port 8081

  gcloud compute http-health-checks create fancy-be-products-hc \
  --request-path /api/products \
  --port 8082

  # Create backend services that are the target for load-balanced traffic. The backend services will use the health checks and named ports you created

  gcloud compute backend-services create fancy-fe-frontend \
  --http-health-checks fancy-fe-frontend-hc \
  --port-name frontend \
  --global

  gcloud compute backend-services create fancy-be-orders \
  --http-health-checks fancy-be-orders-hc \
  --port-name orders \
  --global

  gcloud compute backend-services create fancy-be-products \
  --http-health-checks fancy-be-products-hc \
  --port-name products \
  --global

  # Add the Load Balancer's backend services:

  gcloud compute backend-services add-backend fancy-fe-frontend \
  --instance-group-zone=$ZONE \
  --instance-group fancy-fe-mig \
  --global

  gcloud compute backend-services add-backend fancy-be-orders \
  --instance-group-zone=$ZONE \
  --instance-group fancy-be-mig \
  --global

  gcloud compute backend-services add-backend fancy-be-products \
  --instance-group-zone=$ZONE \
  --instance-group fancy-be-mig \
  --global

  # Create a URL map. The URL map defines which URLs are directed to which backend services:
  gcloud compute url-maps create fancy-map \
  --default-service fancy-fe-frontend

  # Create a path matcher to allow the /api/orders and /api/products paths to route to their respective services:
  gcloud compute url-maps add-path-matcher fancy-map \
   --default-service fancy-fe-frontend \
   --path-matcher-name orders \
   --path-rules "/api/orders=fancy-be-orders,/api/products=fancy-be-products"

   # Create the proxy which ties to the URL map:
   gcloud compute target-http-proxies create fancy-proxy \
  --url-map fancy-map

  # Create a global forwarding rule that ties a public IP address and port to the proxy:
  gcloud compute forwarding-rules create fancy-http-rule \
  --global \
  --target-http-proxy fancy-proxy \
  --ports 80

# Update the configuration
# Now that you have a new static IP address, update the code on the frontend to point to this new address instead of the ephemeral address used earlier that pointed to the backend instance.
# In Cloud Shell, change to the react-app folder which houses the .env file that holds the configuration:

cd ~/monolith-to-microservices/react-app/

# Find the IP address for the Load Balancer:
gcloud compute forwarding-rules list --global

# Return to the Cloud Shell Editor and edit the .env file again to point to Public IP of Load Balancer. [LB_IP] represents the External IP address of the backend instance determined above.
# Rebuild react-app, which will update the frontend code:
cd ~/monolith-to-microservices/react-app
npm install && npm run-script build

# Copy the application code into your bucket:
cd ~
rm -rf monolith-to-microservices/*/node_modules
gsutil -m cp -r monolith-to-microservices gs://fancy-store-$DEVSHELL_PROJECT_ID/

# Update the frontend instances
# Since your instances pull the code at startup, you can issue a rolling restart command:
gcloud compute instance-groups managed rolling-action replace fancy-fe-mig \
    --zone=$ZONE \
    --max-unavailable 100%
# Test the website
#  Run the following to confirm the service is listed as HEALTHY:

watch -n 2 gcloud compute backend-services get-health fancy-fe-frontend --global

# Scaling Compute Engine
# To create the autoscaling policy, execute the following:
gcloud compute instance-groups managed set-autoscaling \
  fancy-fe-mig \
  --zone=$ZONE \
  --max-num-replicas 2 \
  --target-load-balancing-utilization 0.60

  gcloud compute instance-groups managed set-autoscaling \
  fancy-be-mig \
  --zone=$ZONE \
  --max-num-replicas 2 \
  --target-load-balancing-utilization 0.60

# Enable content delivery network
  gcloud compute backend-services update fancy-fe-frontend \
    --enable-cdn --global

# Update the website
# Updating instance template
# Run the following command to modify the machine type of the frontend instance:
gcloud compute instances set-machine-type frontend \
  --zone=$ZONE \
  --machine-type e2-small

# Create the new Instance Template:
gcloud compute instance-templates create fancy-fe-new \
    --region=$REGION \
    --source-instance=frontend \
    --source-instance-zone=$ZONE

# Roll out the updated instance template to the Managed Instance Group:
gcloud compute instance-groups managed rolling-action start-update fancy-fe-mig \
  --zone=$ZONE \
  --version template=fancy-fe-new

# Wait 3 minutes, and then run the following to monitor the status of the update:
watch -n 2 gcloud compute instance-groups managed list-instances fancy-fe-mig \
  --zone=$ZONE

# Run the following to see if the virtual machine is using the new machine type (e2-small), where [VM_NAME] is the newly created instance:
gcloud compute instances describe [VM_NAME] --zone=$ZONE | grep machineType

# Make changes to the website
# Run the following commands to copy the updated file to the correct file name:
cd ~/monolith-to-microservices/react-app/src/pages/Home
mv index.js.new index.js

# Print the file contents to verify the changes:
cat ~/monolith-to-microservices/react-app/src/pages/Home/index.js

# Run the following command to build the React app and copy it into the monolith public directory:
cd ~/monolith-to-microservices/react-app
npm install && npm run-script build

# Then re-push this code to the bucket:
cd ~
rm -rf monolith-to-microservices/*/node_modules
gsutil -m cp -r monolith-to-microservices gs://fancy-store-$DEVSHELL_PROJECT_ID/

# Push changes with rolling replacements
gcloud compute instance-groups managed rolling-action replace fancy-fe-mig \
  --zone=$ZONE \
  --max-unavailable=100%

# Wait 3 minutes after issuing the rolling-action replace command in order to give the instances time to be processed, and then Run the following to confirm the service is listed as HEALTHY:
watch -n 2 gcloud compute backend-services get-health fancy-fe-frontend --global
# Wait a few moments for both services to appear and become HEALTHY.
# Once items appear in the list with HEALTHY status, exit the watch command by pressing CTRL+C
# Browse to the website via http://[LB_IP] where [LB_IP] is the IP_ADDRESS specified for the Load Balancer, which can be found with the following command:
# Simulate failure
# In order to confirm the health check works, log in to an instance and stop the services
# To find an instance name, execute the following:

gcloud compute instance-groups list-instances fancy-fe-mig --zone=$ZONE

# Copy an instance name, then run the following to secure shell into the instance, where INSTANCE_NAME is one of the instances from the list:

gcloud compute ssh fancy-fe-j53x --zone=$ZONE

# Type in "y" to confirm, and press Enter twice to not use a password
# Within the instance, use supervisorctl to stop the application:

sudo supervisorctl stop nodeapp; sudo killall node

# Exit the instance:

exit

# Monitor the repair operations:

watch -n 2 gcloud compute operations list \
--filter='operationType~compute.instances.repair.*'

# It should give an output that shows done, showing that the managed instance recreated the instance to repair it.

# You can also go to Navigation menu > Compute Engine > VM instances to monitor through the Console.


